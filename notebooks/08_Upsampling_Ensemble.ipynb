{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¦· Dental Implant 10-Year Survival Prediction\n",
        "\n",
        "## Notebook 08: Upsampling + 10-Fold LightGBM Ensemble\n",
        "\n",
        "**Objective:** Implement the winning approach from a top competitor:\n",
        "1. **Upsample minority class** (failures) to match majority class\n",
        "2. **10-fold Stratified Cross-Validation** with LightGBM\n",
        "3. **Ensemble predictions** by averaging probabilities from all 10 models\n",
        "4. **Submit probabilities** (not binary predictions)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”‘ Key Techniques:\n",
        "- **Bootstrap Upsampling**: Duplicate minority class samples to create 50/50 balance\n",
        "- **10-Fold Ensemble**: Average predictions from 10 models for robustness\n",
        "- **Probability Output**: Submit survival probabilities instead of 0/1\n",
        "\n",
        "### ðŸ“Š Expected Results:\n",
        "Based on the reference notebook:\n",
        "- **Failure Recall: ~93-97%** (vs our previous 0%!)\n",
        "- **Overall Accuracy: ~91-93%**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 1. Setup & Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "from lightgbm import LGBMClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Periospot Brand Colors\n",
        "COLORS = {\n",
        "    'periospot_blue': '#15365a',\n",
        "    'mystic_blue': '#003049',\n",
        "    'periospot_red': '#6c1410',\n",
        "    'crimson_blaze': '#a92a2a',\n",
        "    'vanilla_cream': '#f7f0da',\n",
        "    'black': '#000000',\n",
        "    'white': '#ffffff',\n",
        "    'periospot_yellow': '#ffc430',\n",
        "}\n",
        "\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.facecolor'] = COLORS['vanilla_cream']\n",
        "plt.rcParams['figure.facecolor'] = COLORS['white']\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 2. Load Raw Data\n",
        "\n",
        "We'll load the raw data and do minimal preprocessing (like the reference notebook).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "df_train = pd.read_csv('../data/raw/train.csv')\n",
        "df_test = pd.read_csv('../data/raw/test.csv')\n",
        "\n",
        "print(f\"Training data: {df_train.shape}\")\n",
        "print(f\"Test data: {df_test.shape}\")\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(df_train['implant_survival_10y'].value_counts())\n",
        "print(f\"\\nClass imbalance ratio: {df_train['implant_survival_10y'].value_counts()[1] / df_train['implant_survival_10y'].value_counts()[0]:.1f}:1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove patient_id (not predictive)\n",
        "train_clean = df_train.drop(['patient_id'], axis=1)\n",
        "test_clean = df_test.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = train_clean.select_dtypes(include='number').columns.tolist()\n",
        "categorical_cols = train_clean.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Remove target from numerical\n",
        "numerical_cols.remove('implant_survival_10y')\n",
        "\n",
        "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 3. One-Hot Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-Hot Encode categorical columns\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "# Fit and transform training data\n",
        "encoded_train = ohe.fit_transform(train_clean[categorical_cols])\n",
        "encoded_cols = ohe.get_feature_names_out(categorical_cols)\n",
        "encoded_train_df = pd.DataFrame(encoded_train, columns=encoded_cols)\n",
        "\n",
        "# Combine numerical and encoded features\n",
        "df_train_processed = train_clean.drop(columns=categorical_cols)\n",
        "df_train_final = pd.concat([df_train_processed.reset_index(drop=True), encoded_train_df], axis=1)\n",
        "\n",
        "print(f\"Processed training data shape: {df_train_final.shape}\")\n",
        "print(f\"Features: {df_train_final.shape[1] - 1} (excluding target)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 4. Upsample Minority Class (Key Technique!)\n",
        "\n",
        "Instead of SMOTE or class weights, we'll **duplicate minority class samples** to create a perfectly balanced 50/50 dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# UPSAMPLE MINORITY CLASS (FAILURES) TO MATCH MAJORITY CLASS\n",
        "# =============================================================================\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = df_train_final[df_train_final['implant_survival_10y'] == 1]\n",
        "minority_class = df_train_final[df_train_final['implant_survival_10y'] == 0]\n",
        "\n",
        "print(f\"Before upsampling:\")\n",
        "print(f\"  Majority class (Survival): {len(majority_class)}\")\n",
        "print(f\"  Minority class (Failure):  {len(minority_class)}\")\n",
        "\n",
        "# Upsample minority class with replacement\n",
        "minority_upsampled = resample(\n",
        "    minority_class,\n",
        "    replace=True,                          # Sample with replacement\n",
        "    n_samples=len(majority_class),         # Match majority class count\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine for balanced dataset\n",
        "balanced_data = pd.concat([majority_class, minority_upsampled])\n",
        "\n",
        "# Shuffle the balanced data\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nAfter upsampling:\")\n",
        "print(f\"  Total samples: {len(balanced_data)}\")\n",
        "print(f\"  Class distribution:\")\n",
        "print(balanced_data['implant_survival_10y'].value_counts())\n",
        "\n",
        "# Separate features and target\n",
        "X_balanced = balanced_data.drop('implant_survival_10y', axis=1)\n",
        "y_balanced = balanced_data['implant_survival_10y']\n",
        "\n",
        "print(f\"\\nâœ… Balanced dataset created: {X_balanced.shape[0]} samples, {X_balanced.shape[1]} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 5. Train 10-Fold LightGBM Ensemble\n",
        "\n",
        "We'll use 10-fold Stratified Cross-Validation and keep all 10 models for ensemble predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 10-FOLD STRATIFIED CROSS-VALIDATION WITH LIGHTGBM\n",
        "# =============================================================================\n",
        "\n",
        "# Initialize LightGBM with default parameters (like the reference notebook)\n",
        "lgb = LGBMClassifier(random_state=42, verbose=-1)\n",
        "\n",
        "# 10-fold Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Run cross-validation and return all estimators\n",
        "print(\"Training 10-fold LightGBM ensemble...\")\n",
        "cv_results = cross_validate(\n",
        "    lgb, \n",
        "    X_balanced, \n",
        "    y_balanced, \n",
        "    cv=skf, \n",
        "    scoring='roc_auc', \n",
        "    n_jobs=-1, \n",
        "    return_estimator=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… 10 models trained!\")\n",
        "print(f\"Mean ROC-AUC: {cv_results['test_score'].mean():.4f} (+/- {cv_results['test_score'].std():.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EVALUATE EACH FOLD\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORTS FOR EACH FOLD\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for fold, model in enumerate(cv_results['estimator']):\n",
        "    # Get validation indices for this fold\n",
        "    val_idx = list(skf.split(X_balanced, y_balanced))[fold][1]\n",
        "    X_val, y_val = X_balanced.iloc[val_idx], y_balanced.iloc[val_idx]\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_val)\n",
        "    \n",
        "    # Print report\n",
        "    print(f\"\\nFold {fold+1}:\")\n",
        "    print(classification_report(y_val, y_pred, target_names=['Failure', 'Survival']))\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 6. Generate Test Predictions (Ensemble Averaging)\n",
        "\n",
        "We'll average the **probabilities** from all 10 models for the final prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREPARE TEST DATA (same preprocessing as training)\n",
        "# =============================================================================\n",
        "\n",
        "# One-hot encode test categorical columns (using same encoder)\n",
        "test_encoded = ohe.transform(test_clean[categorical_cols])\n",
        "test_encoded_df = pd.DataFrame(test_encoded, columns=encoded_cols)\n",
        "\n",
        "# Combine numerical and encoded features\n",
        "df_test_processed = test_clean.drop(columns=categorical_cols)\n",
        "df_test_final = pd.concat([df_test_processed.reset_index(drop=True), test_encoded_df], axis=1)\n",
        "\n",
        "print(f\"Test data shape: {df_test_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENSEMBLE PREDICTIONS (AVERAGE PROBABILITIES FROM ALL 10 MODELS)\n",
        "# =============================================================================\n",
        "\n",
        "# Initialize array for predictions from all folds\n",
        "test_preds = np.zeros((len(df_test_final), 10))\n",
        "\n",
        "# Get probability predictions from each model\n",
        "for fold, model in enumerate(cv_results['estimator']):\n",
        "    test_preds[:, fold] = model.predict_proba(df_test_final)[:, 1]  # Probability of class 1 (Survival)\n",
        "\n",
        "# Average predictions across all 10 models\n",
        "final_probs = test_preds.mean(axis=1)\n",
        "\n",
        "print(f\"âœ… Ensemble predictions generated!\")\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Min probability:  {final_probs.min():.4f}\")\n",
        "print(f\"  Max probability:  {final_probs.max():.4f}\")\n",
        "print(f\"  Mean probability: {final_probs.mean():.4f}\")\n",
        "print(f\"  Std probability:  {final_probs.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize probability distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.hist(final_probs, bins=50, color=COLORS['periospot_blue'], edgecolor='white', alpha=0.7)\n",
        "ax.axvline(x=0.5, color=COLORS['crimson_blaze'], linestyle='--', linewidth=2, label='Threshold 0.5')\n",
        "ax.set_xlabel('Survival Probability', fontweight='bold')\n",
        "ax.set_ylabel('Frequency', fontweight='bold')\n",
        "ax.set_title('Distribution of Ensemble Predictions (10-Model Average)', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/ensemble_prediction_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Count predictions above/below threshold\n",
        "print(f\"\\nPredictions above 0.5 (Survival): {(final_probs >= 0.5).sum()} ({(final_probs >= 0.5).mean()*100:.1f}%)\")\n",
        "print(f\"Predictions below 0.5 (Failure):  {(final_probs < 0.5).sum()} ({(final_probs < 0.5).mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 7. Create Submission File\n",
        "\n",
        "We'll submit **probabilities** (not binary 0/1) as the reference notebook did.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission with probabilities\n",
        "submission = pd.DataFrame({\n",
        "    'patient_id': df_test['patient_id'],\n",
        "    'implant_survival_10y': final_probs\n",
        "})\n",
        "\n",
        "submission.to_csv('../submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: ../submission.csv\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(submission.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### âœ… Upsampling + Ensemble Complete!\n",
        "\n",
        "**Techniques Used:**\n",
        "1. âœ… **Bootstrap Upsampling** - Balanced data to 50/50\n",
        "2. âœ… **10-Fold Stratified CV** - Robust model evaluation\n",
        "3. âœ… **LightGBM Ensemble** - 10 models averaged\n",
        "4. âœ… **Probability Predictions** - Not binary 0/1\n",
        "\n",
        "**Expected Improvement:**\n",
        "- Previous best: 0.92171 (XGBoost)\n",
        "- Reference notebook achieved ~93% with this approach!\n",
        "\n",
        "**Submit to Kaggle:**\n",
        "```bash\n",
        "kaggle competitions submit -c dental-implant-10-year-survival-prediction -f submission.csv -m \"Upsampling + 10-fold LightGBM Ensemble\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
