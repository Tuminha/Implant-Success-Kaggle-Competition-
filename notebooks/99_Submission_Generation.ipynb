{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü¶∑ Dental Implant 10-Year Survival Prediction\n",
        "\n",
        "## Notebook 99: Submission Generation\n",
        "\n",
        "**Objective:** Use the best-performing model (CatBoost with Balanced class weights) to generate predictions on the test set and create the final submission file for Kaggle.\n",
        "\n",
        "---\n",
        "\n",
        "### üèÜ Best Model: CatBoost (Balanced)\n",
        "- **Failure Recall:** 52.8% (Best among all models!)\n",
        "- **Uses native categorical handling** for better performance\n",
        "- **auto_class_weights='Balanced'** for class imbalance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé® Setup: Import Libraries & Configure Plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Periospot Brand Colors\n",
        "COLORS = {\n",
        "    'periospot_blue': '#15365a',\n",
        "    'mystic_blue': '#003049',\n",
        "    'periospot_red': '#6c1410',\n",
        "    'crimson_blaze': '#a92a2a',\n",
        "    'vanilla_cream': '#f7f0da',\n",
        "    'black': '#000000',\n",
        "    'white': '#ffffff',\n",
        "    'classic_periospot_blue': '#0031af',\n",
        "    'periospot_light_blue': '#0297ed',\n",
        "    'periospot_dark_blue': '#02011e',\n",
        "    'periospot_yellow': '#ffc430',\n",
        "    'periospot_bright_blue': '#1040dd'\n",
        "}\n",
        "\n",
        "periospot_palette = [COLORS['periospot_blue'], COLORS['crimson_blaze'], \n",
        "                     COLORS['periospot_light_blue'], COLORS['periospot_yellow'],\n",
        "                     COLORS['mystic_blue'], COLORS['periospot_red']]\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['figure.facecolor'] = COLORS['white']\n",
        "plt.rcParams['axes.facecolor'] = COLORS['vanilla_cream']\n",
        "plt.rcParams['axes.edgecolor'] = COLORS['periospot_blue']\n",
        "\n",
        "sns.set_palette(periospot_palette)\n",
        "\n",
        "print(\"‚úÖ Libraries imported and plotting style configured!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 1. Load and Compare All Model Results\n",
        "\n",
        "First, let's load the results from all the models we trained and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load all model results from the /results/ folder\n",
        "\n",
        "results_files = glob.glob('../results/*.json')\n",
        "all_results = []\n",
        "\n",
        "for file in results_files:\n",
        "    with open(file, 'r') as f:\n",
        "        result = json.load(f)\n",
        "        all_results.append(result)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(all_results)\n",
        "comparison_df = comparison_df.sort_values('roc_auc', ascending=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL COMPARISON - SORTED BY ROC-AUC\")\n",
        "print(\"=\" * 60)\n",
        "print(comparison_df[['model', 'roc_auc', 'accuracy']].to_string(index=False))\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC-AUC comparison\n",
        "ax1 = axes[0]\n",
        "bars1 = ax1.barh(comparison_df['model'], comparison_df['roc_auc'], color=periospot_palette[:len(comparison_df)])\n",
        "ax1.set_xlabel('ROC-AUC Score')\n",
        "ax1.set_title('Model Comparison - ROC-AUC', fontweight='bold')\n",
        "ax1.bar_label(bars1, fmt='%.4f', padding=3)\n",
        "ax1.set_xlim(0, 1.0)\n",
        "\n",
        "# Accuracy comparison\n",
        "ax2 = axes[1]\n",
        "bars2 = ax2.barh(comparison_df['model'], comparison_df['accuracy'], color=periospot_palette[:len(comparison_df)])\n",
        "ax2.set_xlabel('Accuracy')\n",
        "ax2.set_title('Model Comparison - Accuracy', fontweight='bold')\n",
        "ax2.bar_label(bars2, fmt='%.4f', padding=3)\n",
        "ax2.set_xlim(0, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/all_models_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the best model\n",
        "best_model_name = comparison_df.iloc[0]['model']\n",
        "best_roc_auc = comparison_df.iloc[0]['roc_auc']\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"   ROC-AUC Score: {best_roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 2. Load Data & Train Best Model on Full Dataset\n",
        "\n",
        "Now we'll train the best-performing model on the ENTIRE training dataset (not just the training split) to maximize performance on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD CATBOOST-SPECIFIC DATA (with native categorical handling)\n",
        "# =============================================================================\n",
        "\n",
        "# Load CatBoost training data (categorical columns kept as strings)\n",
        "X = pd.read_csv('../data/processed/catboost/X_train_catboost.csv')\n",
        "y = pd.read_csv('../data/processed/catboost/y_train_catboost.csv').values.ravel()\n",
        "\n",
        "# Load CatBoost test data\n",
        "X_test = pd.read_csv('../data/processed/catboost/X_test_catboost.csv')\n",
        "test_ids = pd.read_csv('../data/processed/catboost/test_ids_catboost.csv')\n",
        "\n",
        "# Load categorical column names\n",
        "cat_features = pd.read_csv('../data/processed/catboost/categorical_columns.csv')['categorical_columns'].tolist()\n",
        "\n",
        "print(f\"Training data shape: {X.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Test IDs shape: {test_ids.shape}\")\n",
        "print(f\"\\nCategorical columns ({len(cat_features)}): {cat_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN CATBOOST (BALANCED) ON FULL DATASET\n",
        "# =============================================================================\n",
        "# Based on our comparison, CatBoost with auto_class_weights='Balanced' \n",
        "# achieved the best failure recall (52.8%)\n",
        "\n",
        "best_model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        "    cat_features=cat_features,  # Native categorical handling\n",
        "    auto_class_weights='Balanced'  # Handle class imbalance\n",
        ")\n",
        "\n",
        "# Train on FULL dataset (not just training split)\n",
        "best_model.fit(X, y)\n",
        "\n",
        "print(\"‚úÖ CatBoost (Balanced) trained on full dataset!\")\n",
        "print(f\"   Training samples: {len(X)}\")\n",
        "print(f\"   Features: {X.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 3. Generate Predictions & Create Submission File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENERATE PREDICTIONS ON TEST SET\n",
        "# =============================================================================\n",
        "\n",
        "test_predictions = best_model.predict(X_test)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(test_predictions)} predictions\")\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "print(pd.Series(test_predictions).value_counts())\n",
        "print(f\"\\nSurvival rate in predictions: {(test_predictions == 1).mean()*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD SAMPLE SUBMISSION FORMAT\n",
        "# =============================================================================\n",
        "\n",
        "sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
        "\n",
        "print(\"Sample Submission Format:\")\n",
        "print(sample_submission.head())\n",
        "print(f\"\\nExpected shape: {sample_submission.shape}\")\n",
        "print(f\"Columns: {list(sample_submission.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE AND SAVE SUBMISSION FILE\n",
        "# =============================================================================\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_ids['patient_id'],\n",
        "    'implant_survival_10y': test_predictions\n",
        "})\n",
        "\n",
        "submission_df.to_csv('../submission.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Submission file created: ../submission.csv\")\n",
        "print(f\"\\nShape: {submission_df.shape}\")\n",
        "print(submission_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### ‚úÖ Submission Generation Complete!\n",
        "\n",
        "**Model Used:** CatBoost (Balanced)\n",
        "- Native categorical handling for better performance\n",
        "- auto_class_weights='Balanced' for class imbalance\n",
        "- 52.8% failure recall on validation set\n",
        "\n",
        "**How to Submit to Kaggle:**\n",
        "\n",
        "**Via UI:** Go to Kaggle ‚Üí Competition ‚Üí Submit Predictions ‚Üí Upload `submission.csv`\n",
        "\n",
        "**Via CLI:**\n",
        "```bash\n",
        "kaggle competitions submit -c dental-implant-10-year-survival-prediction -f submission.csv -m \"CatBoost Balanced - 52.8% failure recall\"\n",
        "```\n",
        "\n",
        "ü¶∑ Good luck with your submission!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Initialize and train your best-performing model on the ENTIRE training dataset.\n",
        "# Based on the comparison above, choose the appropriate model.\n",
        "\n",
        "# Example: If XGBoost was the best model:\n",
        "# best_model = xgb.XGBClassifier(\n",
        "#     n_estimators=100,\n",
        "#     max_depth=6,\n",
        "#     learning_rate=0.1,\n",
        "#     random_state=42,\n",
        "#     eval_metric='auc',\n",
        "#     use_label_encoder=False\n",
        "# )\n",
        "\n",
        "# Example: If LightGBM was the best:\n",
        "# best_model = lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, verbose=-1)\n",
        "\n",
        "# Example: If CatBoost was the best:\n",
        "# best_model = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=False)\n",
        "\n",
        "# TODO: Choose and initialize your best model based on results\n",
        "best_model = ...\n",
        "\n",
        "# TODO: Fit the model on the entire training dataset\n",
        "# best_model.fit(X, y)\n",
        "...\n",
        "\n",
        "print(f\"‚úÖ {best_model_name} trained on full dataset!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 3. Generate Predictions & Create Submission File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Make predictions on the test set.\n",
        "# You can predict class labels (0 or 1) or probabilities depending on competition requirements.\n",
        "\n",
        "# For class labels:\n",
        "test_predictions = best_model.predict(X_test)\n",
        "\n",
        "# For probabilities (use this if the competition requires probability scores):\n",
        "# test_predictions_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"Generated {len(test_predictions)} predictions\")\n",
        "print(f\"Prediction distribution:\")\n",
        "print(pd.Series(test_predictions).value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load the sample_submission.csv file to see the expected format.\n",
        "sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
        "\n",
        "print(\"Sample Submission Format:\")\n",
        "print(sample_submission.head())\n",
        "print(f\"\\nExpected shape: {sample_submission.shape}\")\n",
        "print(f\"Columns: {list(sample_submission.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create the submission dataframe with predictions.\n",
        "\n",
        "# Option 1: If the sample_submission has an ID column and a target column\n",
        "submission_df = sample_submission.copy()\n",
        "submission_df['implant_survival_10y'] = test_predictions\n",
        "\n",
        "# Option 2: Create from scratch using test_ids\n",
        "# submission_df = pd.DataFrame({\n",
        "#     'id': test_ids.values.ravel(),\n",
        "#     'implant_survival_10y': test_predictions\n",
        "# })\n",
        "\n",
        "print(\"Submission DataFrame:\")\n",
        "print(submission_df.head())\n",
        "print(f\"\\nShape: {submission_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Save the final submission file.\n",
        "\n",
        "submission_df.to_csv('../submission.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Submission file created successfully!\")\n",
        "print(f\"   Location: ../submission.csv\")\n",
        "print(f\"   Shape: {submission_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 4. Validate Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate the submission file\n",
        "\n",
        "submission_check = pd.read_csv('../submission.csv')\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"SUBMISSION VALIDATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check shape matches sample submission\n",
        "shape_match = submission_check.shape == sample_submission.shape\n",
        "print(f\"‚úì Shape matches sample: {shape_match} ({submission_check.shape})\")\n",
        "\n",
        "# Check columns match\n",
        "columns_match = list(submission_check.columns) == list(sample_submission.columns)\n",
        "print(f\"‚úì Columns match: {columns_match} ({list(submission_check.columns)})\")\n",
        "\n",
        "# Check for missing values\n",
        "no_missing = submission_check.isnull().sum().sum() == 0\n",
        "print(f\"‚úì No missing values: {no_missing}\")\n",
        "\n",
        "# Check prediction values are valid (0 or 1 for classification)\n",
        "valid_values = submission_check['implant_survival_10y'].isin([0, 1]).all()\n",
        "print(f\"‚úì Valid prediction values (0 or 1): {valid_values}\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if all([shape_match, columns_match, no_missing, valid_values]):\n",
        "    print(\"\\nüéâ SUBMISSION FILE IS VALID!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è SUBMISSION FILE HAS ISSUES - Please check!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 5. Upload Instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Submit to Kaggle\n",
        "\n",
        "**Option 1: Via Kaggle UI**\n",
        "1. Go to the [Kaggle Competition Page](https://www.kaggle.com/competitions/dental-implant-10-year-survival-prediction)\n",
        "2. Click on \"Submit Predictions\"\n",
        "3. Upload your `submission.csv` file\n",
        "4. Add a description of your submission (e.g., \"XGBoost with default parameters\")\n",
        "\n",
        "**Option 2: Via Kaggle CLI**\n",
        "```bash\n",
        "kaggle competitions submit -c dental-implant-10-year-survival-prediction -f submission.csv -m \"Best model submission - XGBoost\"\n",
        "```\n",
        "\n",
        "**Note:** Make sure you have the Kaggle API key configured if using the CLI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### ‚úÖ Submission Generation Complete!\n",
        "\n",
        "**Summary:**\n",
        "- Compared all trained models\n",
        "- Selected the best performing model based on ROC-AUC\n",
        "- Trained on full dataset for maximum performance\n",
        "- Generated and validated submission file\n",
        "\n",
        "**Files created:**\n",
        "- `submission.csv` - Ready for Kaggle upload\n",
        "\n",
        "**Next Steps (Optional):**\n",
        "- [ ] Hyperparameter tuning with GridSearchCV/Optuna\n",
        "- [ ] Feature engineering iterations\n",
        "- [ ] Ensemble multiple models\n",
        "- [ ] Cross-validation for more robust evaluation\n",
        "\n",
        "ü¶∑ Good luck with your submission!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
